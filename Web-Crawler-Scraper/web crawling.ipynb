{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e73c4e63-3d4a-4ba4-851a-c987c1a4cf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISS Location Data:\n",
      "{'message': 'success', 'iss_position': {'latitude': '48.9143', 'longitude': '-162.4197'}, 'timestamp': 1718478078}\n"
     ]
    }
   ],
   "source": [
    "# Import the requests library\n",
    "import requests\n",
    "\n",
    "# Define the URL for the ISS (International Space Station) location API\n",
    "URL = \"http://api.open-notify.org/iss-now.json\"\n",
    "\n",
    "# Send a GET request to the API and store the response\n",
    "response = requests.get(URL)\n",
    "\n",
    "# Check if the request was successful (status code 200 indicates success)\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "\n",
    "    # Print the parsed data (ISS location details)\n",
    "    print(\"ISS Location Data:\")\n",
    "    print(data)\n",
    "else:\n",
    "    print(\n",
    "        f\"Error: Failed to retrieve data. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27cb66a2-552c-47dd-bcac-4d33ec278e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image downloaded successfully as gfg_logo.png\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "image_url = \"https://media.geeksforgeeks.org/wp-content/uploads/20230505175603/100-Days-of-Machine-Learning.webp\"\n",
    "output_filename = \"gfg_logo.png\"\n",
    "\n",
    "response = requests.get(image_url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(output_filename, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"Image downloaded successfully as {output_filename}\")\n",
    "else:\n",
    "    print(\"Failed to download the image.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbeefacd-4920-4b67-a098-f4c0f72cb5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current temperature is: 19\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "import requests\n",
    "\n",
    "weather_url = \"https://weather.com/en-IN/weather/today/l/a3f9aea9bbe14043f5b2120b83645de05bd267209969dc8de9170098127d5fc9\"\n",
    "\n",
    "response = requests.get(weather_url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    dom = etree.HTML(response.text)\n",
    "    elements = dom.xpath(\n",
    "        \"//span[@data-testid='TemperatureValue' and contains(@class,'CurrentConditions')]\")\n",
    "    \n",
    "    if elements:\n",
    "        temperature = elements[0].text\n",
    "        print(f\"The current temperature is: {temperature}\")\n",
    "    else:\n",
    "        print(\"Temperature element not found.\")\n",
    "else:\n",
    "    print(\"Failed to fetch the webpage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efe5ea47-344d-46ec-94c5-f5064fb2eea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling: https://books.toscrape.com/\n",
      "Crawling: https://books.toscrape.com/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books_1/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/travel_2/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/mystery_3/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/classics_6/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/philosophy_7/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/romance_8/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/womens-fiction_9/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/fiction_10/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/childrens_11/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/religion_12/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/music_14/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/default_15/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/science-fiction_16/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/sports-and-games_17/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/add-a-comment_18/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/fantasy_19/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/new-adult_20/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/young-adult_21/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/science_22/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/poetry_23/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/paranormal_24/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/art_25/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/psychology_26/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/autobiography_27/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/parenting_28/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/adult-fiction_29/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/humor_30/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/horror_31/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/history_32/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/food-and-drink_33/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/christian-fiction_34/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/business_35/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/biography_36/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/thriller_37/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/contemporary_38/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/spirituality_39/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/academic_40/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/self-help_41/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/historical_42/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/christian_43/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/suspense_44/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/short-stories_45/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/novels_46/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/health_47/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/politics_48/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/cultural_49/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/erotica_50/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books/crime_51/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/soumission_998/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/sharp-objects_997/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/the-requiem-red_995/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/the-dirty-little-secrets-of-getting-your-dream-job_994/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/the-black-maria_991/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/starving-hearts-triangular-trade-trilogy-1_990/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/shakespeares-sonnets_989/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/set-me-free_988/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/rip-it-up-and-start-again_986/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/olio_984/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/libertarianism-for-beginners_982/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/its-only-the-himalayas_981/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/page-2.html\n",
      "Crawling: https://books.toscrape.com/catalogue/category/books_1/page-2.html\n",
      "Crawling: https://books.toscrape.com/catalogue/full-moon-over-noahs-ark-an-odyssey-to-mount-ararat-and-beyond_811/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/see-america-a-celebration-of-our-national-parks-treasured-sites_732/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/vagabonding-an-uncommon-guide-to-the-art-of-long-term-world-travel_552/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/under-the-tuscan-sun_504/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/a-summer-in-europe_458/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/the-great-railway-bazaar_446/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/a-year-in-provence-provence-1_421/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/the-road-to-little-dribbling-adventures-of-an-american-in-britain-notes-from-a-small-island-2_277/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/neither-here-nor-there-travels-in-europe_198/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/1000-places-to-see-before-you-die_1/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/in-a-dark-dark-wood_963/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/the-past-never-ends_942/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/a-murder-in-time_877/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/the-murder-of-roger-ackroyd-hercule-poirot-4_852/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/the-last-mile-amos-decker-2_754/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/that-darkness-gardiner-and-renner-1_743/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/tastes-like-fear-di-marnie-rome-3_742/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/a-time-of-torment-charlie-parker-14_657/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/a-study-in-scarlet-sherlock-holmes-1_656/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/poisonous-max-revere-novels-3_627/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/murder-at-the-42nd-street-library-raymond-ambler-1_624/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/most-wanted_623/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/hide-away-eve-duncan-20_620/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/boar-island-anna-pigeon-19_613/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/the-widow_609/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/playing-with-fire_602/index.html\n",
      "Crawling: https://books.toscrape.com/catalogue/what-happened-on-beale-street-secrets-of-the-south-mysteries-2_506/index.html\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 43\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrawling: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 43\u001b[0m new_links \u001b[38;5;241m=\u001b[39m crawl_page(current_url)\n\u001b[0;32m     44\u001b[0m visited_urls\u001b[38;5;241m.\u001b[39madd(current_url)\n\u001b[0;32m     45\u001b[0m urls_to_visit\u001b[38;5;241m.\u001b[39mextend(new_links)\n",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m, in \u001b[0;36mcrawl_page\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcrawl_page\u001b[39m(url):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m         response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m     18\u001b[0m         response\u001b[38;5;241m.\u001b[39mraise_for_status()  \u001b[38;5;66;03m# Raise an exception for HTTP errors\u001b[39;00m\n\u001b[0;32m     20\u001b[0m         soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3_2\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3_2\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3_2\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3_2\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3_2\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32m~\\anaconda3_2\\Lib\\site-packages\\urllib3\\connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    792\u001b[0m     conn,\n\u001b[0;32m    793\u001b[0m     method,\n\u001b[0;32m    794\u001b[0m     url,\n\u001b[0;32m    795\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    796\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    797\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    798\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    799\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    800\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    801\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    802\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    803\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    804\u001b[0m )\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    807\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3_2\\Lib\\site-packages\\urllib3\\connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m~\\anaconda3_2\\Lib\\site-packages\\urllib3\\connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    460\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32m~\\anaconda3_2\\Lib\\http\\client.py:1386\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1385\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1386\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1387\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1388\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3_2\\Lib\\http\\client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3_2\\Lib\\http\\client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3_2\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3_2\\Lib\\ssl.py:1315\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1312\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1313\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1314\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\anaconda3_2\\Lib\\ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# URL of the website to crawl\n",
    "base_url = \"https://books.toscrape.com/\"\n",
    "\n",
    "# Set to store visited URLs\n",
    "visited_urls = set()\n",
    "\n",
    "# List to store URLs to visit next\n",
    "urls_to_visit = [base_url]\n",
    "\n",
    "# Function to crawl a page and extract links\n",
    "def crawl_page(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        # Extract links and enqueue new URLs\n",
    "        links = []\n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            next_url = urljoin(url, link[\"href\"])\n",
    "            links.append(next_url)\n",
    "        \n",
    "        return links\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error crawling {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Crawl the website\n",
    "while urls_to_visit:\n",
    "    current_url = urls_to_visit.pop(0)  # Dequeue the first URL\n",
    "\n",
    "    if current_url in visited_urls:\n",
    "        continue\n",
    "\n",
    "    print(f\"Crawling: {current_url}\")\n",
    "\n",
    "    new_links = crawl_page(current_url)\n",
    "    visited_urls.add(current_url)\n",
    "    urls_to_visit.extend(new_links)\n",
    "\n",
    "print(\"Crawling finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62690e34-3b24-443c-99ec-40eef31dd2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
